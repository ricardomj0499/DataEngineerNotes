,Property Name,Default,Meaning,Since Version
0,spark.app.name,(none),"
    The name of your application. This will appear in the UI and in log data.
  ",0.9.0
1,spark.driver.cores,1,"
    Number of cores to use for the driver process, only in cluster mode.
  ",1.3.0
2,spark.driver.maxResultSize,1g,"
    Limit of total size of serialized results of all partitions for each Spark action (e.g.
    collect) in bytes. Should be at least 1M, or 0 for unlimited. Jobs will be aborted if the total
    size is above this limit.
    Having a high limit may cause out-of-memory errors in driver (depends on spark.driver.memory
    and memory overhead of objects in JVM). Setting a proper limit can protect the driver from
    out-of-memory errors.
  ",1.2.0
3,spark.driver.memory,1g,"
    Amount of memory to use for the driver process, i.e. where SparkContext is initialized, in the
    same format as JVM memory strings with a size unit suffix (""k"", ""m"", ""g"" or ""t"")
    (e.g. 512m, 2g).
    
Note: In client mode, this config must not be set through the SparkConf
    directly in your application, because the driver JVM has already started at that point.
    Instead, please set this through the --driver-memory command line option
    or in your default properties file.
  ",1.1.1
4,spark.driver.memoryOverhead,"driverMemory * spark.driver.memoryOverheadFactor, with minimum of 384 ","
    Amount of non-heap memory to be allocated per driver process in cluster mode, in MiB unless
    otherwise specified. This is memory that accounts for things like VM overheads, interned strings,
    other native overheads, etc. This tends to grow with the container size (typically 6-10%).
    This option is currently supported on YARN, Mesos and Kubernetes.
    Note: Non-heap memory includes off-heap memory
    (when spark.memory.offHeap.enabled=true) and memory used by other driver processes
    (e.g. python process that goes with a PySpark driver) and memory used by other non-driver
    processes running in the same container. The maximum memory size of container to running
    driver is determined by the sum of spark.driver.memoryOverhead
    and spark.driver.memory.
  ",2.3.0
5,spark.driver.memoryOverheadFactor,0.10,"
    Fraction of driver memory to be allocated as additional non-heap memory per driver process in cluster mode.
    This is memory that accounts for things like VM overheads, interned strings,
    other native overheads, etc. This tends to grow with the container size.
    This value defaults to 0.10 except for Kubernetes non-JVM jobs, which defaults to
    0.40. This is done as non-JVM tasks need more non-JVM heap space and such tasks
    commonly fail with ""Memory Overhead Exceeded"" errors. This preempts this error
    with a higher default.
    This value is ignored if spark.driver.memoryOverhead is set directly.
  ",3.3.0
6,spark.driver.resource.{resourceName}.amount,0,"
    Amount of a particular resource type to use on the driver.
    If this is used, you must also specify the
    spark.driver.resource.{resourceName}.discoveryScript
    for the driver to find the resource on startup.
  ",3.0.0
7,spark.driver.resource.{resourceName}.discoveryScript,None,"
    A script for the driver to run to discover a particular resource type. This should
    write to STDOUT a JSON string in the format of the ResourceInformation class. This has a
    name and an array of addresses. For a client-submitted driver, discovery script must assign
    different resource addresses to this driver comparing to other drivers on the same host.
  ",3.0.0
8,spark.driver.resource.{resourceName}.vendor,None,"
    Vendor of the resources to use for the driver. This option is currently
    only supported on Kubernetes and is actually both the vendor and domain following
    the Kubernetes device plugin naming convention. (e.g. For GPUs on Kubernetes
    this config would be set to nvidia.com or amd.com)
  ",3.0.0
9,spark.resources.discoveryPlugin,org.apache.spark.resource.ResourceDiscoveryScriptPlugin,"
    Comma-separated list of class names implementing
    org.apache.spark.api.resource.ResourceDiscoveryPlugin to load into the application.
    This is for advanced users to replace the resource discovery class with a
    custom implementation. Spark will try each class specified until one of them
    returns the resource information for that resource. It tries the discovery
    script last if none of the plugins return information for that resource.
  ",3.0.0
10,spark.executor.memory,1g,"
    Amount of memory to use per executor process, in the same format as JVM memory strings with
    a size unit suffix (""k"", ""m"", ""g"" or ""t"") (e.g. 512m, 2g).
  ",0.7.0
11,spark.executor.pyspark.memory,Not set,"
    The amount of memory to be allocated to PySpark in each executor, in MiB
    unless otherwise specified.  If set, PySpark memory for an executor will be
    limited to this amount. If not set, Spark will not limit Python's memory use
    and it is up to the application to avoid exceeding the overhead memory space
    shared with other non-JVM processes. When PySpark is run in YARN or Kubernetes, this memory
    is added to executor resource requests.
    
Note: This feature is dependent on Python's `resource` module; therefore, the behaviors and
    limitations are inherited. For instance, Windows does not support resource limiting and actual
    resource is not limited on MacOS.
  ",2.4.0
12,spark.executor.memoryOverhead,"executorMemory * spark.executor.memoryOverheadFactor, with minimum of 384 ","
    Amount of additional memory to be allocated per executor process, in MiB unless otherwise specified.
    This is memory that accounts for things like VM overheads, interned strings, other native overheads, etc.
    This tends to grow with the executor size (typically 6-10%). This option is currently supported on YARN and Kubernetes.
    
Note: Additional memory includes PySpark executor memory
    (when spark.executor.pyspark.memory is not configured) and memory used by other
    non-executor processes running in the same container. The maximum memory size of container to
    running executor is determined by the sum of spark.executor.memoryOverhead,
    spark.executor.memory, spark.memory.offHeap.size and
    spark.executor.pyspark.memory.
  ",2.3.0
13,spark.executor.memoryOverheadFactor,0.10,"
    Fraction of executor memory to be allocated as additional non-heap memory per executor process.
    This is memory that accounts for things like VM overheads, interned strings,
    other native overheads, etc. This tends to grow with the container size.
    This value defaults to 0.10 except for Kubernetes non-JVM jobs, which defaults to
    0.40. This is done as non-JVM tasks need more non-JVM heap space and such tasks
    commonly fail with ""Memory Overhead Exceeded"" errors. This preempts this error
    with a higher default.
    This value is ignored if spark.executor.memoryOverhead is set directly.
  ",3.3.0
14,spark.executor.resource.{resourceName}.amount,0,"
    Amount of a particular resource type to use per executor process.
    If this is used, you must also specify the
    spark.executor.resource.{resourceName}.discoveryScript
    for the executor to find the resource on startup.
  ",3.0.0
15,spark.executor.resource.{resourceName}.discoveryScript,None,"
    A script for the executor to run to discover a particular resource type. This should
    write to STDOUT a JSON string in the format of the ResourceInformation class. This has a
    name and an array of addresses.
  ",3.0.0
16,spark.executor.resource.{resourceName}.vendor,None,"
    Vendor of the resources to use for the executors. This option is currently
    only supported on Kubernetes and is actually both the vendor and domain following
    the Kubernetes device plugin naming convention. (e.g. For GPUs on Kubernetes
    this config would be set to nvidia.com or amd.com)
  ",3.0.0
17,spark.extraListeners,(none),"
    A comma-separated list of classes that implement SparkListener; when initializing
    SparkContext, instances of these classes will be created and registered with Spark's listener
    bus.  If a class has a single-argument constructor that accepts a SparkConf, that constructor
    will be called; otherwise, a zero-argument constructor will be called. If no valid constructor
    can be found, the SparkContext creation will fail with an exception.
  ",1.3.0
18,spark.local.dir,/tmp,"
    Directory to use for ""scratch"" space in Spark, including map output files and RDDs that get
    stored on disk. This should be on a fast, local disk in your system. It can also be a
    comma-separated list of multiple directories on different disks.

    
Note: This will be overridden by SPARK_LOCAL_DIRS (Standalone), MESOS_SANDBOX (Mesos) or
    LOCAL_DIRS (YARN) environment variables set by the cluster manager.
  ",0.5.0
19,spark.logConf,false,"
    Logs the effective SparkConf as INFO when a SparkContext is started.
  ",0.9.0
20,spark.master,(none),"
    The cluster manager to connect to. See the list of
     allowed master URL's.
  ",0.9.0
21,spark.submit.deployMode,client,"
    The deploy mode of Spark driver program, either ""client"" or ""cluster"",
    Which means to launch driver program locally (""client"")
    or remotely (""cluster"") on one of the nodes inside the cluster.
  ",1.5.0
22,spark.log.callerContext,(none),"
    Application information that will be written into Yarn RM log/HDFS audit log when running on Yarn/HDFS.
    Its length depends on the Hadoop configuration hadoop.caller.context.max.size. It should be concise,
    and typically can have up to 50 characters.
  ",2.2.0
23,spark.log.level,(none),"
    When set, overrides any user-defined log settings as if calling
    SparkContext.setLogLevel() at Spark startup. Valid log levels include: ""ALL"", ""DEBUG"", ""ERROR"", ""FATAL"", ""INFO"", ""OFF"", ""TRACE"", ""WARN"".
  ",3.5.0
24,spark.driver.supervise,false,"
    If true, restarts the driver automatically if it fails with a non-zero exit status.
    Only has effect in Spark standalone mode or Mesos cluster deploy mode.
  ",1.3.0
25,spark.driver.log.dfsDir,(none),"
    Base directory in which Spark driver logs are synced, if spark.driver.log.persistToDfs.enabled
    is true. Within this base directory, each application logs the driver logs to an application specific file.
    Users may want to set this to a unified location like an HDFS directory so driver log files can be persisted
    for later usage. This directory should allow any Spark user to read/write files and the Spark History Server
    user to delete files. Additionally, older logs from this directory are cleaned by the
    Spark History Server if
    spark.history.fs.driverlog.cleaner.enabled is true and, if they are older than max age configured
    by setting spark.history.fs.driverlog.cleaner.maxAge.
  ",3.0.0
26,spark.driver.log.persistToDfs.enabled,false,"
    If true, spark application running in client mode will write driver logs to a persistent storage, configured
    in spark.driver.log.dfsDir. If spark.driver.log.dfsDir is not configured, driver logs
    will not be persisted. Additionally, enable the cleaner by setting spark.history.fs.driverlog.cleaner.enabled
    to true in Spark History Server.
  ",3.0.0
27,spark.driver.log.layout,%d{yy/MM/dd HH:mm:ss.SSS} %t %p %c{1}: %m%n%ex,"
    The layout for the driver logs that are synced to spark.driver.log.dfsDir. If this is not configured,
    it uses the layout for the first appender defined in log4j2.properties. If that is also not configured, driver logs
    use the default layout.
  ",3.0.0
28,spark.driver.log.allowErasureCoding,false,"
    Whether to allow driver logs to use erasure coding.  On HDFS, erasure coded files will not
    update as quickly as regular replicated files, so they make take longer to reflect changes
    written by the application. Note that even if this is true, Spark will still not force the
    file to use erasure coding, it will simply use file system defaults.
  ",3.0.0
29,spark.decommission.enabled,false,"
    When decommission enabled, Spark will try its best to shut down the executor gracefully.
    Spark will try to migrate all the RDD blocks (controlled by spark.storage.decommission.rddBlocks.enabled)
    and shuffle blocks (controlled by spark.storage.decommission.shuffleBlocks.enabled) from the decommissioning
    executor to a remote executor when spark.storage.decommission.enabled is enabled.
    With decommission enabled, Spark will also decommission an executor instead of killing when spark.dynamicAllocation.enabled enabled.
  ",3.1.0
30,spark.executor.decommission.killInterval,(none),"
    Duration after which a decommissioned executor will be killed forcefully by an outside (e.g. non-spark) service.
  ",3.1.0
31,spark.executor.decommission.forceKillTimeout,(none),"
    Duration after which a Spark will force a decommissioning executor to exit.
    This should be set to a high value in most situations as low values will prevent block migrations from having enough time to complete.
  ",3.2.0
32,spark.executor.decommission.signal,PWR,"
    The signal that used to trigger the executor to start decommission.
  ",3.2.0
33,spark.executor.maxNumFailures,"numExecutors * 2, with minimum of 3","
    The maximum number of executor failures before failing the application.
    This configuration only takes effect on YARN, or Kubernetes when 
    `spark.kubernetes.allocation.pods.allocator` is set to 'direct'.
  ",3.5.0
34,spark.executor.failuresValidityInterval,(none),"
    Interval after which executor failures will be considered independent and
    not accumulate towards the attempt count.
    This configuration only takes effect on YARN, or Kubernetes when 
    `spark.kubernetes.allocation.pods.allocator` is set to 'direct'.
  ",3.5.0
